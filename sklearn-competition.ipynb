{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/amsamms/sklearn-competition?scriptVersionId=106005661\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown","outputs":[],"execution_count":0},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom time import time\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.base import clone","metadata":{"execution":{"iopub.status.busy":"2022-09-07T08:50:59.674013Z","iopub.execute_input":"2022-09-07T08:50:59.674508Z","iopub.status.idle":"2022-09-07T08:50:59.682598Z","shell.execute_reply.started":"2022-09-07T08:50:59.674469Z","shell.execute_reply":"2022-09-07T08:50:59.681172Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X= pd.read_csv('../input/data-science-london-scikit-learn/train.csv',header=None)","metadata":{"execution":{"iopub.status.busy":"2022-09-07T08:39:08.732011Z","iopub.execute_input":"2022-09-07T08:39:08.732514Z","iopub.status.idle":"2022-09-07T08:39:08.787362Z","shell.execute_reply.started":"2022-09-07T08:39:08.732474Z","shell.execute_reply":"2022-09-07T08:39:08.786068Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y= pd.read_csv('../input/data-science-london-scikit-learn/trainLabels.csv',header=None)","metadata":{"execution":{"iopub.status.busy":"2022-09-07T08:39:25.976103Z","iopub.execute_input":"2022-09-07T08:39:25.976659Z","iopub.status.idle":"2022-09-07T08:39:25.986205Z","shell.execute_reply.started":"2022-09-07T08:39:25.976612Z","shell.execute_reply":"2022-09-07T08:39:25.984898Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y=y[0]","metadata":{"execution":{"iopub.status.busy":"2022-09-07T08:39:30.100081Z","iopub.execute_input":"2022-09-07T08:39:30.102073Z","iopub.status.idle":"2022-09-07T08:39:30.120473Z","shell.execute_reply.started":"2022-09-07T08:39:30.10199Z","shell.execute_reply":"2022-09-07T08:39:30.119152Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y.shape","metadata":{"execution":{"iopub.status.busy":"2022-09-07T08:39:38.28445Z","iopub.execute_input":"2022-09-07T08:39:38.285119Z","iopub.status.idle":"2022-09-07T08:39:38.299368Z","shell.execute_reply.started":"2022-09-07T08:39:38.285067Z","shell.execute_reply":"2022-09-07T08:39:38.297962Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X.shape","metadata":{"execution":{"iopub.status.busy":"2022-09-07T08:39:53.952403Z","iopub.execute_input":"2022-09-07T08:39:53.952933Z","iopub.status.idle":"2022-09-07T08:39:53.962819Z","shell.execute_reply.started":"2022-09-07T08:39:53.952896Z","shell.execute_reply":"2022-09-07T08:39:53.960941Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x=5","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X.isnull().sum().sum()","metadata":{"execution":{"iopub.status.busy":"2022-09-07T08:41:54.908017Z","iopub.execute_input":"2022-09-07T08:41:54.908605Z","iopub.status.idle":"2022-09-07T08:41:54.922927Z","shell.execute_reply.started":"2022-09-07T08:41:54.90853Z","shell.execute_reply":"2022-09-07T08:41:54.921409Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we are sure that X is 2 dimensional np array and y is 1 dimensional np array and there is no missing data, lets check normal distribution of independent variables X","metadata":{}},{"cell_type":"code","source":"for column in X.columns:\n    X[column].hist()\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-09-07T08:42:04.163516Z","iopub.execute_input":"2022-09-07T08:42:04.163975Z","iopub.status.idle":"2022-09-07T08:42:13.06077Z","shell.execute_reply.started":"2022-09-07T08:42:04.163941Z","shell.execute_reply":"2022-09-07T08:42:13.059507Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"All 40 columns are normally distributed, hence logistic regression and support vector machine, works just fine, also i will try two ensble methods, adaboost and randomforest","metadata":{}},{"cell_type":"code","source":"X.describe()","metadata":{"execution":{"iopub.status.busy":"2022-09-07T08:43:36.564388Z","iopub.execute_input":"2022-09-07T08:43:36.56496Z","iopub.status.idle":"2022-09-07T08:43:36.695659Z","shell.execute_reply.started":"2022-09-07T08:43:36.564917Z","shell.execute_reply":"2022-09-07T08:43:36.693819Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Also, all data seems to have close range, no need for using `MINMAXSCALER()`\n\nNext, lets define a function that takes a predefined number of estimators and train-test split, fit and predict for a specefic number of times, and compare the results\n\nNote: the following function is not very simple, as it can used any where, so i wrote it in a generic way","metadata":{}},{"cell_type":"code","source":"def estimators_repeater(estimators=[RandomForestClassifier(),AdaBoostClassifier(),SVC()],tr_slicer=(None,None),tst_slicer=(None,None),loops=500,scorer=accuracy_score,X=X,y=y):\n    '''This function aims to train list of supplied estimators with selcted slices of datasets for as many time as required(default 500)\n    and then produce a list of training score, test score and time used for each estimator\n    \n    - It is important to import all used estimators, score to be used\n    inputs :\n    - estimators : a list of estimators, deafult is Randomforest, Adaboost and support vector machine\n    \n    - tr_slicer : slicer for the number of observations needed in the training, default is all samples [:], slicer should be tuples\n    of integers(starter,ender) default is(None,None)\n    \n    - tst_slicer : slicer for the number of samples to be tested at, default is all samples [0:-1], slicer should be tuples\n    of integers(starter,ender) default is(0,-1)\n    \n    -loops : int, is a number of loops needed : default 500\n    \n    -scorer : default is accuracy_score, but it can be anything choosen from sklearn.metrics but if it is something calculated by\n    methods other than accuracy, it should be modified in the \n    \n    -X= features in the form of dataframe or np.array of 2 dimensions\n    -y= target in the form of dataframe, series or np.array\n    \n    Returns : 3 global dataframes for training score(training_score_df),\n    testing score(testing_score_df) and time (time_df) used for each estimator for fitting and predicting\n    \n    \n    Example, fitting first 200 samples for SVC() and RandomForestClassifier() for 400 loop and scorer is accuracy for full test dataset:\n    \n    estimators_repeater(estimators=[RandomForestClassifier(),SVC()],tr_slicer=(0,200),loops=400,scorer=accuracy_score,X=X,y=y)\n    \n    '''\n    \n    training_score={}\n    testing_score={}\n    timing={}\n    for clf in estimators:\n        clf_name = clf.__class__.__name__\n        training_score[clf_name]=[]\n        testing_score[clf_name]=[]\n        timing[clf_name]=[]\n       \n    for i in range (loops):\n        k1=time()\n        X_train, X_test, y_train, y_test = train_test_split(X,y,random_state=i)\n        for clf in estimators:\n            a=time()\n            clf_name = clf.__class__.__name__\n            clean_clf=clone(clf)\n            clean_clf.fit(X_train[tr_slicer[0]:tr_slicer[1]],y_train[tr_slicer[0]:tr_slicer[1]])\n            training_score[clf_name].append(scorer(y_train[tr_slicer[0]:tr_slicer[1]],clean_clf.predict(X_train[tr_slicer[0]:tr_slicer[1]])))\n            testing_score[clf_name].append(scorer(y_test[tst_slicer[0]:tst_slicer[1]],clean_clf.predict(X_test[tst_slicer[0]:tst_slicer[1]])))\n            b=time()\n            timing[clf_name].append(b-a)\n        k2=time()\n        print(f'loop number {i} out of {loops} took {k2-k1} seconds')\n    \n    global training_score_df\n    training_score_df=pd.DataFrame(training_score)\n    global testing_score_df\n    testing_score_df=pd.DataFrame(testing_score)\n    global timing_df\n    timing_df=pd.DataFrame(timing)","metadata":{"execution":{"iopub.status.busy":"2022-09-07T08:49:20.033651Z","iopub.execute_input":"2022-09-07T08:49:20.034981Z","iopub.status.idle":"2022-09-07T08:49:20.0556Z","shell.execute_reply.started":"2022-09-07T08:49:20.034891Z","shell.execute_reply":"2022-09-07T08:49:20.054396Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"A simple apply to the above function would be below as follows:\n\n* test split data \n* fit X, and y for randomforest, adaboost, SVC and logestic regression\n* record the time for the above steps, also record test data score and record train data score\n* Do the above 500 times\n* the final output would be 3 dataframes, one for the test_Scores, and one for train scores, and last one for time consumed for each loop","metadata":{}},{"cell_type":"code","source":"estimators_repeater(estimators=[RandomForestClassifier(), AdaBoostClassifier(), SVC(),LogisticRegression()],X=X,y=y)","metadata":{"execution":{"iopub.status.busy":"2022-09-07T08:51:05.769956Z","iopub.execute_input":"2022-09-07T08:51:05.770409Z","iopub.status.idle":"2022-09-07T08:58:56.057317Z","shell.execute_reply.started":"2022-09-07T08:51:05.770376Z","shell.execute_reply":"2022-09-07T08:58:56.055427Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"training_score_df.describe()","metadata":{"execution":{"iopub.status.busy":"2022-09-07T08:59:18.06452Z","iopub.execute_input":"2022-09-07T08:59:18.065074Z","iopub.status.idle":"2022-09-07T08:59:18.099297Z","shell.execute_reply.started":"2022-09-07T08:59:18.065021Z","shell.execute_reply":"2022-09-07T08:59:18.097922Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It seems for training score, randomforest rocks ( which is expected), folloed by SVC","metadata":{}},{"cell_type":"code","source":"testing_score_df.describe()","metadata":{"execution":{"iopub.status.busy":"2022-09-07T09:06:27.929644Z","iopub.execute_input":"2022-09-07T09:06:27.930896Z","iopub.status.idle":"2022-09-07T09:06:27.96069Z","shell.execute_reply.started":"2022-09-07T09:06:27.930811Z","shell.execute_reply":"2022-09-07T09:06:27.959321Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As for testing score, it seems SVC is the best with average score of 0.894472","metadata":{}},{"cell_type":"code","source":"timing_df.describe()","metadata":{"execution":{"iopub.status.busy":"2022-09-07T09:08:12.34052Z","iopub.execute_input":"2022-09-07T09:08:12.341075Z","iopub.status.idle":"2022-09-07T09:08:12.370957Z","shell.execute_reply.started":"2022-09-07T09:08:12.341037Z","shell.execute_reply":"2022-09-07T09:08:12.369693Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"it seems randomforest classifier took average of 0.5 second per fit,predict cycle, while adaboost took 0.3 second while SVC took 0.06 second ! ","metadata":{}},{"cell_type":"markdown","source":"So i will use SVC for final submission","metadata":{}},{"cell_type":"code","source":"testing_score_df.sort_values(by='SVC',ascending=False)","metadata":{"execution":{"iopub.status.busy":"2022-09-07T09:11:31.950218Z","iopub.execute_input":"2022-09-07T09:11:31.951909Z","iopub.status.idle":"2022-09-07T09:11:31.971442Z","shell.execute_reply.started":"2022-09-07T09:11:31.951852Z","shell.execute_reply":"2022-09-07T09:11:31.969874Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# initiate SVC instance with random_state that get the most svc score from the predefined function\nprediction=SVC(random_state=303)","metadata":{"execution":{"iopub.status.busy":"2022-09-07T09:13:04.998363Z","iopub.execute_input":"2022-09-07T09:13:04.998928Z","iopub.status.idle":"2022-09-07T09:13:05.006237Z","shell.execute_reply.started":"2022-09-07T09:13:04.998888Z","shell.execute_reply":"2022-09-07T09:13:05.004972Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# fitting whole test.csv and trainlabel.csv\nprediction.fit(X,y)","metadata":{"execution":{"iopub.status.busy":"2022-09-07T09:13:33.32095Z","iopub.execute_input":"2022-09-07T09:13:33.321437Z","iopub.status.idle":"2022-09-07T09:13:33.369214Z","shell.execute_reply.started":"2022-09-07T09:13:33.321401Z","shell.execute_reply":"2022-09-07T09:13:33.368174Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# defining test data that will be predicted for submission\ntest_data= pd.read_csv('../input/data-science-london-scikit-learn/test.csv',header=None)","metadata":{"execution":{"iopub.status.busy":"2022-09-07T09:14:01.916785Z","iopub.execute_input":"2022-09-07T09:14:01.917285Z","iopub.status.idle":"2022-09-07T09:14:02.135156Z","shell.execute_reply.started":"2022-09-07T09:14:01.917248Z","shell.execute_reply":"2022-09-07T09:14:02.133805Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions=prediction.predict(test_data)","metadata":{"execution":{"iopub.status.busy":"2022-09-07T09:14:14.237175Z","iopub.execute_input":"2022-09-07T09:14:14.237736Z","iopub.status.idle":"2022-09-07T09:14:14.644074Z","shell.execute_reply.started":"2022-09-07T09:14:14.237696Z","shell.execute_reply":"2022-09-07T09:14:14.642857Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# constructing of submission dataframe\ndf=pd.DataFrame()\ndf['Id']=[i for i in range(1,9001)]\ndf['Solution']=predictions","metadata":{"execution":{"iopub.status.busy":"2022-09-07T09:18:35.711372Z","iopub.execute_input":"2022-09-07T09:18:35.711842Z","iopub.status.idle":"2022-09-07T09:18:35.729039Z","shell.execute_reply.started":"2022-09-07T09:18:35.711806Z","shell.execute_reply":"2022-09-07T09:18:35.727074Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.tail()","metadata":{"execution":{"iopub.status.busy":"2022-09-07T09:18:51.083822Z","iopub.execute_input":"2022-09-07T09:18:51.084284Z","iopub.status.idle":"2022-09-07T09:18:51.096592Z","shell.execute_reply.started":"2022-09-07T09:18:51.084249Z","shell.execute_reply":"2022-09-07T09:18:51.094826Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.to_csv('submission.csv',index=False)","metadata":{"execution":{"iopub.status.busy":"2022-09-07T09:22:11.831644Z","iopub.execute_input":"2022-09-07T09:22:11.832077Z","iopub.status.idle":"2022-09-07T09:22:11.850146Z","shell.execute_reply.started":"2022-09-07T09:22:11.832042Z","shell.execute_reply":"2022-09-07T09:22:11.849044Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Final submission get me **0.89819** which consider good, as i didn't use gridsearch","metadata":{}}]}